{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(\"recipes.csv\")\n",
    "\n",
    "# Delete rows where the value in the \"Images\" column is \"character(0)\"\n",
    "df = df[df[\"Images\"] != \"character(0)\"]\n",
    "\n",
    "# Write the updated DataFrame back to a new CSV file\n",
    "df.to_csv(\"updated_recipes.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(\"updated_recipes.csv\")\n",
    "\n",
    "# Display the data types of the columns mentioned in the warning\n",
    "mixed_columns = ['column_name', 'dtype']\n",
    "\n",
    "for col in df.columns[[0, 2, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]]:\n",
    "    mixed_columns.append([col, df[col].dtype])\n",
    "\n",
    "print(mixed_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(\"recipe.csv\")\n",
    "\n",
    "# Count the number of empty values in the \"RecipeCategory\" column\n",
    "empty_rows_count = df['RecipeCategory'].isna().sum()\n",
    "\n",
    "print(\"Number of empty rows in the RecipeCategory column:\", empty_rows_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(\"recipe.csv\")\n",
    "\n",
    "# Count the number of empty values in the \"RecipeCategory\" column\n",
    "empty_rows_count = df['RecipeCategory'].isna().sum()\n",
    "\n",
    "# Delete rows where the \"RecipeCategory\" column is empty\n",
    "df = df.dropna(subset=['RecipeCategory'])\n",
    "\n",
    "# Write the updated DataFrame back to the same CSV file (overwriting the original file)\n",
    "df.to_csv(\"recipe.csv\", index=False)\n",
    "\n",
    "print(\"Number of empty rows deleted:\", empty_rows_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(\"recipe.csv\")\n",
    "\n",
    "# List of columns to be deleted\n",
    "columns_to_delete = ['RecipeId', 'AuthorId', 'AuthorName', 'CookTime', 'PrepTime', 'TotalTime', 'DatePublished', 'AggregatedRating', 'ReviewCount', 'SaturatedFatContent', 'RecipeServings', 'RecipeYield']\n",
    "\n",
    "# Delete the specified columns\n",
    "df.drop(columns=columns_to_delete, inplace=True)\n",
    "\n",
    "# Write the updated DataFrame back to the same CSV file (overwriting the original file)\n",
    "df.to_csv(\"recipe.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(\"recipe.csv\")\n",
    "\n",
    "# Count the number of empty values in the \"RecipeCategory\" column\n",
    "empty_rows_count = df['RecipeInstructions', 'Name', 'Description', ].isna().sum()\n",
    "\n",
    "print(\"Number of empty rows in the RecipeInstructions column:\", empty_rows_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(\"recipe.csv\")\n",
    "\n",
    "# Count the number of empty values in the \"RecipeCategory\" column\n",
    "empty_rows_count = df['RecipeInstructions'].isna().sum()\n",
    "\n",
    "# Delete rows where the \"RecipeCategory\" column is empty\n",
    "df = df.dropna(subset=['RecipeInstructions'])\n",
    "\n",
    "# Write the updated DataFrame back to the same CSV file (overwriting the original file)\n",
    "df.to_csv(\"recipe.csv\", index=False)\n",
    "\n",
    "print(\"Number of empty rows deleted:\", empty_rows_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(\"recipe.csv\")\n",
    "\n",
    "# List of columns to check for empty rows\n",
    "columns_to_check = ['Name', 'Description', 'Images', 'RecipeCategory', 'Keywords', 'RecipeIngredientQuantities', 'RecipeIngredientParts', 'Calories', 'FatContent', 'CholesterolContent', 'SodiumContent', 'CarbohydrateContent', 'FiberContent', 'SugarContent', 'ProteinContent', 'RecipeInstructions']\n",
    "\n",
    "# Count the number of empty values in each column\n",
    "empty_rows_count = df[columns_to_check].isna().sum()\n",
    "\n",
    "print(\"Number of empty rows in each column:\")\n",
    "print(empty_rows_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(\"recipe.csv\")\n",
    "\n",
    "# Filter the DataFrame for rows where the \"Keywords\" column is empty\n",
    "empty_keywords_rows = df[df['Keywords'].isna()]\n",
    "\n",
    "# Extract the \"Name\" from the \"Name\" column for each empty row\n",
    "names_for_empty_keywords = empty_keywords_rows['Name']\n",
    "\n",
    "print(\"Names from the 'Name' column corresponding to empty rows in the 'Keywords' column:\")\n",
    "print(names_for_empty_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(\"recipe.csv\")\n",
    "\n",
    "# Count the number of empty rows in the \"Keywords\" column\n",
    "empty_keywords_rows_count = df['Keywords'].isna().sum()\n",
    "\n",
    "# Delete the empty rows from the \"Keywords\" column\n",
    "df = df.dropna(subset=['Keywords'])\n",
    "\n",
    "# Write the updated DataFrame back to the same CSV file (overwriting the original file)\n",
    "df.to_csv(\"recipe.csv\", index=False)\n",
    "\n",
    "print(\"Number of empty rows deleted from the 'Keywords' column:\", empty_keywords_rows_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import missingno as msno\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(\"recipe.csv\")\n",
    "\n",
    "# Visualize missing data\n",
    "msno.matrix(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(\"recipe.csv\")\n",
    "\n",
    "# Check for missing values in the DataFrame\n",
    "missing_data = df.isnull().sum()\n",
    "\n",
    "print(\"Number of missing values in each column:\")\n",
    "print(missing_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(\"recipe.csv\")\n",
    "\n",
    "# Count the initial number of rows\n",
    "initial_row_count = len(df)\n",
    "\n",
    "# Drop rows with any missing data\n",
    "df = df.dropna()\n",
    "\n",
    "# Count the number of rows after deletion\n",
    "final_row_count = len(df)\n",
    "\n",
    "# Calculate the number of rows deleted\n",
    "rows_deleted = initial_row_count - final_row_count\n",
    "\n",
    "# Write the updated DataFrame back to the same CSV file (overwriting the original file)\n",
    "df.to_csv(\"recipe.csv\", index=False)\n",
    "\n",
    "print(\"Number of rows with missing data deleted:\", rows_deleted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import concurrent.futures\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('recipe.csv')\n",
    "\n",
    "# Function to extract URLs from the string\n",
    "def extract_urls(row):\n",
    "    urls = re.findall(r'https?://\\S+?\\.jpg', row)\n",
    "    return urls\n",
    "\n",
    "# Apply the function to each row\n",
    "df['urls'] = df['Images'].apply(extract_urls)\n",
    "\n",
    "# Flatten the list of URLs\n",
    "urls_list = [url for sublist in df['urls'].tolist() for url in sublist]\n",
    "\n",
    "# Function to check if URL is working\n",
    "def check_url(url):\n",
    "    try:\n",
    "        response = requests.head(url, timeout=10)  # Set a timeout to avoid waiting too long for non-responsive URLs\n",
    "        return response.status_code == 200\n",
    "    except (requests.ConnectionError, requests.Timeout):\n",
    "        return False\n",
    "\n",
    "# Define the number of threads (adjust according to your system's capabilities)\n",
    "num_threads = 20\n",
    "\n",
    "# Split the URLs into chunks for parallel processing\n",
    "url_chunks = [urls_list[i:i + len(urls_list) // num_threads] for i in range(0, len(urls_list), len(urls_list) // num_threads)]\n",
    "\n",
    "# Function to process URLs using multithreading\n",
    "def process_urls(chunk):\n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        futures = {executor.submit(check_url, url): url for url in chunk}\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            url = futures[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "                results.append((url, result))\n",
    "            except Exception as exc:\n",
    "                results.append((url, False))  # Mark URL as not working in case of any exception\n",
    "    return results\n",
    "\n",
    "# Process URLs and print the results\n",
    "for chunk in url_chunks:\n",
    "    results = process_urls(chunk)\n",
    "    for url, status in results:\n",
    "        if status:\n",
    "            print(f\"{url} is working\")\n",
    "        else:\n",
    "            print(f\"{url} is not working\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('recipe.csv')\n",
    "\n",
    "# Function to extract URLs from the string\n",
    "def extract_urls(row):\n",
    "    urls = re.findall(r'https?://\\S+?\\.jpg', row)\n",
    "    return urls\n",
    "\n",
    "# Apply the function to each row\n",
    "df['urls'] = df['Images'].apply(extract_urls)\n",
    "\n",
    "# Flatten the list of URLs\n",
    "urls_list = [url for sublist in df['urls'].tolist() for url in sublist]\n",
    "\n",
    "# Function to check if URL is working\n",
    "def check_url(url):\n",
    "    try:\n",
    "        response = requests.head(url)\n",
    "        return response.status_code == 200\n",
    "    except requests.ConnectionError:\n",
    "        return False\n",
    "\n",
    "# Check if URLs are working\n",
    "for url in urls_list:\n",
    "    if check_url(url):\n",
    "        print(f\"{url} is working\")\n",
    "    else:\n",
    "        print(f\"{url} is not working\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('Dataset.csv')\n",
    "\n",
    "# Function to extract only the first URL from the string\n",
    "def extract_first_url(row):\n",
    "    urls = re.findall(r'https?://\\S+?\\.jpg', row)\n",
    "    if urls:\n",
    "        return urls[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Apply the function to each row\n",
    "df['url'] = df['Images'].apply(extract_first_url)\n",
    "\n",
    "# Drop the 'Images' column if needed\n",
    "df = df.drop(columns=['Images'])\n",
    "\n",
    "# Remove rows with no URL\n",
    "df = df.dropna(subset=['url'])\n",
    "\n",
    "# Function to check if URL is working\n",
    "def check_url(url):\n",
    "    try:\n",
    "        response = requests.head(url)\n",
    "        return response.status_code == 200\n",
    "    except requests.ConnectionError:\n",
    "        return False\n",
    "\n",
    "# Check if URLs are working\n",
    "for url in df['url']:\n",
    "    if check_url(url):\n",
    "        print(f\"{url} is working\")\n",
    "    else:\n",
    "        print(f\"{url} is not working\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('Modified_Data.csv')\n",
    "\n",
    "# Function to process keywords\n",
    "def process_keywords(row):\n",
    "    # Extract keywords from the row\n",
    "    keywords = re.findall(r'\"([^\"]*)\"', row)\n",
    "    # Join keywords and convert to lowercase\n",
    "    processed_keywords = ','.join(keyword.lower().replace(' ', '') for keyword in keywords)\n",
    "    return processed_keywords\n",
    "\n",
    "# Apply the function to each row\n",
    "df['Proceed_RecipeIngredientParts'] = df['RecipeIngredientParts'].apply(process_keywords)\n",
    "\n",
    "# Write the processed data to a new CSV file\n",
    "df.to_csv('Modified_Data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('Recipes_Processed.csv')\n",
    "\n",
    "# Function to process keywords\n",
    "def process_keywords(row):\n",
    "    # Remove integers, '<', '>', and '&' signs\n",
    "    processed_keywords = re.sub(r'\\d+', '', row)  # Remove integers\n",
    "    processed_keywords = processed_keywords.replace('<', '').replace('>', '').replace('&', '').replace('mins', '').replace('hours', '').replace('.', '').replace('/', '')\n",
    "    return processed_keywords\n",
    "\n",
    "# Apply the function to each row\n",
    "df['Proceed_RecipeIngredientParts'] = df['Proceed_RecipeIngredientParts'].apply(process_keywords)\n",
    "\n",
    "# Write the processed data to a new CSV file\n",
    "df.to_csv('Recipes_Processed.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('Recipes_Processed.csv')\n",
    "\n",
    "# Function to process keywords\n",
    "def process_keywords(row):\n",
    "    try:\n",
    "        if pd.isna(row):  # Check if the value is NaN\n",
    "            return None\n",
    "\n",
    "        # Convert to string and remove leading and trailing whitespace\n",
    "        row = str(row).strip()\n",
    "\n",
    "        # Check if row is empty or contains only commas\n",
    "        if not row or row == ',':\n",
    "            return None\n",
    "\n",
    "        # Remove leading commas\n",
    "        processed_keywords = row.lstrip(',')\n",
    "\n",
    "        # Replace double commas with single comma\n",
    "        processed_keywords = processed_keywords.replace(',,', ',')\n",
    "\n",
    "        # Remove trailing comma if present\n",
    "        if processed_keywords.endswith(','):\n",
    "            processed_keywords = processed_keywords[:-1]\n",
    "\n",
    "        return processed_keywords\n",
    "    except AttributeError:\n",
    "        return None\n",
    "\n",
    "# Count rows before processing\n",
    "rows_before = len(df)\n",
    "\n",
    "# Apply the function to each row\n",
    "df['Proceed_RecipeIngredientParts'] = df['Proceed_RecipeIngredientParts'].apply(process_keywords)\n",
    "\n",
    "# Remove rows with None values (empty rows)\n",
    "df = df.dropna(subset=['Proceed_RecipeIngredientParts'])\n",
    "\n",
    "# Count rows after processing\n",
    "rows_after = len(df)\n",
    "\n",
    "# Calculate the number of deleted rows\n",
    "rows_deleted = rows_before - rows_after\n",
    "\n",
    "# Write the processed data to a new CSV file\n",
    "df.to_csv('Recipes_Processed.csv', index=False)\n",
    "\n",
    "print(f\"Number of deleted rows: {rows_deleted}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('Recipes_Processed.csv')\n",
    "\n",
    "# Count rows before processing\n",
    "rows_before = len(df)\n",
    "\n",
    "# Check for 'character(0)' in each cell\n",
    "rows_to_delete = df[df.apply(lambda row: row.astype(str).str.contains('character\\(0\\)', na=False)).any(axis=1)].index\n",
    "\n",
    "# Delete rows containing 'character(0)'\n",
    "df = df.drop(rows_to_delete)\n",
    "\n",
    "# Count rows after processing\n",
    "rows_after = len(df)\n",
    "\n",
    "# Calculate the number of deleted rows\n",
    "rows_deleted = rows_before - rows_after\n",
    "\n",
    "# Write the processed data to a new CSV file\n",
    "df.to_csv('Recipes_Processed.csv', index=False)\n",
    "\n",
    "print(f\"Number of deleted rows: {rows_deleted}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('Recipes_Processed.csv')\n",
    "\n",
    "# Generate IDs\n",
    "ids = range(1, len(df) + 1)\n",
    "\n",
    "# Insert the IDs column at the beginning of the DataFrame\n",
    "df.insert(0, 'ID', ids)\n",
    "\n",
    "# Write the DataFrame with IDs to a new CSV file\n",
    "df.to_csv('Recipes_Processed.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Read data from CSV file\n",
    "df = pd.read_csv('Recipes_Processed.csv', on_bad_lines='skip')\n",
    "\n",
    "# Preprocess the keywords column\n",
    "keywords_corpus = df['merged_column'].fillna('').astype(str).tolist()\n",
    "\n",
    "# Vectorize the keywords using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(keywords_corpus)\n",
    "\n",
    "# Convert the TF-IDF matrix to DataFrame\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Concatenate the original dataframe with the TF-IDF dataframe\n",
    "result_df = pd.concat([df, tfidf_df], axis=1)\n",
    "\n",
    "# Save the result to a new CSV file\n",
    "result_df.to_csv('DataSet.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('fdata.csv')\n",
    "\n",
    "# Specify the columns to modify\n",
    "columns_to_modify = ['RecipeIngredientQuantities', 'RecipeIngredientParts']\n",
    "\n",
    "# Remove the 'c' from outside the parentheses in the specified columns\n",
    "for column in columns_to_modify:\n",
    "    df[column] = df[column].str.replace(r'(?<!\\()\\bc\\b(?!\\))', '', regex=True)\n",
    "\n",
    "# Write the modified DataFrame back to a new CSV file\n",
    "df.to_csv('final.csv', index=False)\n",
    "\n",
    "print(\"Done! 'c' removed from outside the parentheses in the specified columns in the CSV file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(\"Recipes_Processed.csv\")\n",
    "\n",
    "# Merge the two columns and add commas between elements\n",
    "merged_column = df[\"processed_keywords\"] + \", \" + df[\"Proceed_RecipeIngredientParts\"]\n",
    "\n",
    "# Replace the existing columns with the merged column\n",
    "df[\"merged_column\"] = merged_column\n",
    "\n",
    "\n",
    "# Save the modified DataFrame back to the CSV file\n",
    "df.to_csv(\"Recipes_Processed.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(\"Recipes_Processed.csv\")\n",
    "\n",
    "# Function to remove special characters and numbers\n",
    "def remove_special_characters_and_numbers(text):\n",
    "    # Remove special characters except commas\n",
    "    text = re.sub(r'[^\\w\\s,]', '', text)\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    return text\n",
    "\n",
    "# Apply the function to the merged column\n",
    "df[\"merged_column\"] = df[\"merged_column\"].apply(remove_special_characters_and_numbers)\n",
    "\n",
    "# Save the modified DataFrame back to the CSV file\n",
    "df.to_csv(\"Recipes_Processed.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(\"Recipes_Processed.csv\")\n",
    "\n",
    "# Drop the original columns\n",
    "df.drop(columns=[\"processed_keywords\", \"Proceed_RecipeIngredientParts\"], inplace=True)\n",
    "\n",
    "# Save the modified DataFrame back to the CSV file\n",
    "df.to_csv(\"Recipes_Processed.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('final.csv')\n",
    "\n",
    "# Function to check if any meat or seafood words are present in the text\n",
    "def check_veg_nonveg(text):\n",
    "    # List of meat and seafood words\n",
    "    meat_seafood_words = [\n",
    "        'shrimp', 'crab', 'lobster', 'oysters', 'oyster', 'squid', 'octopus',\n",
    "        'salmon', 'tuna', 'swordfish', 'fish'\n",
    "        'catfish', 'fish', 'anchovy', 'sardine', 'sardines', 'mackerel', 'trout', 'haddock', 'hake', 'halibut', 'cod', 'bass', 'carp', 'perch', 'pike', 'pollock', 'snapper', 'tilapia', 'turbot', 'whitefish', 'mahi-mahi',\n",
    "        'beef', 'pork', 'chicken', 'duck', 'goose', 'meat', 'deer','meats','chickens','porks','beefs','ducks','gooses','deers','turkey', 'lamb', 'veal', 'mutton', 'rabbit', 'goat', 'ostrich', 'kangaroo', 'boar','breast','poultry', 'egg', 'eggs', 'leg', 'legs','hen', 'frog', 'horse', 'camel', 'alligator', 'buffalo', 'snail','crab', 'lobster', 'oysters', 'oyster', 'squid', 'octopus','salmon', 'tuna', 'swordfish', 'fish''catfish', 'fish', 'anchovy', 'sardine', 'sardines', 'mackerel', 'trout', 'haddock', 'hake', 'halibut', 'cod', 'bass', 'carp', 'perch', 'pike', 'pollock', 'snapper', 'tilapia', 'turbot', 'whitefish', 'mahi-mahi','beef', 'pork', 'chicken', 'duck', 'goose', 'meat', 'deer','meats','chickens','porks','beefs','ducks','gooses','deers','turkey', 'lamb', 'veal', 'mutton', 'rabbit', 'goat', 'ostrich', 'kangaroo', 'boar','breast','poultry', 'egg', 'eggs', 'leg', 'legs','hen', 'frog', 'horse', 'camel', 'alligator', 'buffalo', 'snail',\n",
    "        'rabbit', 'goat', 'ostrich', 'kangaroo', 'boar',\n",
    "        'breast',\n",
    "        'poultry', 'egg', 'eggs', 'leg', 'legs',\n",
    "        'hen', 'frog', 'horse', 'camel', 'alligator', 'buffalo', 'snail'\n",
    "    ]\n",
    "\n",
    "    # Check if any meat or seafood words are present in the text\n",
    "    for word in meat_seafood_words:\n",
    "        if word in text:\n",
    "            return 1  # Non-vegetarian\n",
    "    return 0  # Vegetarian\n",
    "\n",
    "# Apply the function to the 'merged_column' column\n",
    "df['Veg/NonVeg'] = df['merged_column'].apply(check_veg_nonveg)\n",
    "\n",
    "# Write the modified DataFrame to the same CSV file\n",
    "df.to_csv('final.csv', index=False)\n",
    "\n",
    "print(\"New CSV file created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(\"final.csv\")\n",
    "\n",
    "# Process the data in the specified columns\n",
    "df['RecipeIngredientQuantities'] = df['RecipeIngredientQuantities'].str.replace(\"(\", \"[\").str.replace(\")\", \"]\")\n",
    "df['RecipeIngredientParts'] = df['RecipeIngredientParts'].str.replace(\"(\", \"[\").str.replace(\")\", \"]\")\n",
    "\n",
    "# Write the processed data back to the same CSV file\n",
    "df.to_csv(\"finallast.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(\"Modified_Data.csv\")\n",
    "\n",
    "# Select the first 100 rows\n",
    "df = df.head(100)\n",
    "\n",
    "# Save the modified DataFrame back to CSV\n",
    "df.to_csv(\"Modified_Data.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def merge_and_format_columns(column1, column2):\n",
    "    merged_strings = [\", \".join([f\"[{c1} {c2}]\" for c1, c2 in zip(col1, col2)]) for col1, col2 in zip(column1, column2)]\n",
    "    return merged_strings\n",
    "\n",
    "# Read data from CSV file and store it in lists\n",
    "column1 = []\n",
    "column2 = []\n",
    "with open('finallast.csv', newline='', encoding='utf-8') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        column1.append(row['RecipeIngredientQuantities'].split(','))\n",
    "        column2.append(row['RecipeIngredientParts'].split(','))\n",
    "\n",
    "# Merge and format the columns\n",
    "merged_output = merge_and_format_columns(column1, column2)\n",
    "\n",
    "# Add the merged data as a new column\n",
    "with open('finallast.csv', 'r', newline='', encoding='utf-8') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    rows = list(reader)\n",
    "\n",
    "with open('finallast.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    fieldnames = reader.fieldnames + ['Ingredient']  # Add a new column for merged data\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()  # Write the header row\n",
    "\n",
    "    for i, row in enumerate(rows):\n",
    "        row['Ingredient'] = merged_output[i]  # Add merged data to the new column\n",
    "        writer.writerow(row)  # Write the updated row to the CSV file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('finallast.csv')\n",
    "\n",
    "# Remove the \" \", [ and ] characters from the MergedColumn\n",
    "df['Ingredient'] = df['Ingredient'].str.replace('\"', '').str.replace('[', '').str.replace(']', '')\n",
    "\n",
    "# Save the modified DataFrame back to a CSV file\n",
    "df.to_csv('finallast.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('finallast.csv')\n",
    "\n",
    "# Remove the 'NA' if it's a separate word in the MergedColumn\n",
    "df['Ingredient'] = df['Ingredient'].str.replace(r'\\bNA\\b', '', regex=True)\n",
    "\n",
    "# Write the modified DataFrame back to a new CSV file\n",
    "df.to_csv('finallast.csv', index=False)\n",
    "\n",
    "print(\"Done! 'NA' removed from the MergedColumn in the CSV file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('finallast.csv')\n",
    "\n",
    "# If the column 'mergedcolumn' exists\n",
    "if 'Ingredient' in df.columns:\n",
    "    # Clean up the 'mergedcolumn' values by replacing multiple spaces with single space\n",
    "    df['Ingredient'] = df['Ingredient'].str.replace(r'\\s+', ' ', regex=True)\n",
    "\n",
    "    # Write the cleaned DataFrame to a new CSV file\n",
    "    df.to_csv('finallast.csv', index=False)\n",
    "else:\n",
    "    print(\"Column 'Ingredient' not found in the input CSV.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('Data.csv')\n",
    "\n",
    "# Merge the 'Name', 'Description', and 'merged_column' columns into the 'merged_column' column\n",
    "df['merged_column'] = df['Name'] + ' '  + ' ' + df['merged_column']\n",
    "\n",
    "# Drop the 'Name' and 'Description' columns if needed\n",
    "# df.drop(['Name', 'Description'], axis=1, inplace=True)\n",
    "\n",
    "# Write the modified DataFrame back to a new CSV file\n",
    "df.to_csv('DataSet.csv', index=False)\n",
    "\n",
    "# Display the modified DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('DataSet.csv')\n",
    "\n",
    "# Convert 'merged_column' to lowercase\n",
    "df['merged_column'] = df['merged_column'].str.lower()\n",
    "\n",
    "# Write the processed data to a new CSV file\n",
    "df.to_csv('DataSet.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('DataSet.csv')\n",
    "\n",
    "# Function to remove special characters and numbers\n",
    "def remove_special_characters(text):\n",
    "    # Remove special characters and numbers except commas\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z,\\s]', '', text)\n",
    "    return cleaned_text\n",
    "\n",
    "# Apply the function to each row in 'merged_column'\n",
    "df['merged_column'] = df['merged_column'].apply(remove_special_characters)\n",
    "\n",
    "# Convert 'merged_column' to lowercase\n",
    "df['merged_column'] = df['merged_column'].str.lower()\n",
    "\n",
    "# Write the processed data to a new CSV file\n",
    "df.to_csv('DataSet.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('tempo.csv')\n",
    "\n",
    "# Function to extract the first URL from the string\n",
    "def extract_first_url(row):\n",
    "    urls = re.findall(r'https?://\\S+?\\.jpg', row)\n",
    "    if urls:\n",
    "        return urls[0]  # Return only the first URL if found\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Apply the function to each row to extract the first URL\n",
    "df['Images'] = df['Images'].apply(extract_first_url)\n",
    "\n",
    "\n",
    "\n",
    "# Save the modified DataFrame to a new CSV file\n",
    "df.to_csv('tempo.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Empty cell check in images\n",
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('tempo.csv')\n",
    "\n",
    "# Check for empty cells in the 'Images' column\n",
    "empty_cells = df['Images'].isnull().sum()\n",
    "\n",
    "if empty_cells > 0:\n",
    "    print(f\"There are {empty_cells} empty cells in the 'Images' column.\")\n",
    "else:\n",
    "    print(\"There are no empty cells in the 'Images' column.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('tempo.csv')\n",
    "\n",
    "# Get the initial number of rows\n",
    "initial_rows = df.shape[0]\n",
    "\n",
    "# Drop rows with any empty cell\n",
    "df.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "# Get the final number of rows\n",
    "final_rows = df.shape[0]\n",
    "\n",
    "# Calculate the number of rows deleted\n",
    "rows_deleted = initial_rows - final_rows\n",
    "\n",
    "print(f\"{rows_deleted} row(s) were deleted due to containing empty cells.\")\n",
    "\n",
    "# Now, df contains the modified DataFrame without any rows containing empty cells\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('tempo.csv')\n",
    "\n",
    "# Count the number of rows before deleting\n",
    "rows_before = df.shape[0]\n",
    "\n",
    "# Drop rows with empty cells in the 'Images' column\n",
    "df.dropna(subset=['Images'], inplace=True)\n",
    "\n",
    "# Count the number of rows after deleting\n",
    "rows_after = df.shape[0]\n",
    "\n",
    "# Calculate the number of rows deleted\n",
    "rows_deleted = rows_before - rows_after\n",
    "\n",
    "print(\"Number of rows deleted:\", rows_deleted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('tempo.csv')\n",
    "\n",
    "# Remove the starting 'c' from each row in the 'RecipeInstructions' column\n",
    "df['RecipeInstructions'] = df['RecipeInstructions'].str.lstrip('c')\n",
    "\n",
    "# Write the modified DataFrame back to a new CSV file\n",
    "df.to_csv('tempo.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('tempo.csv')\n",
    "\n",
    "# Remove the starting '(' and ending ')' from each row in the 'RecipeInstructions' column\n",
    "df['RecipeInstructions'] = df['RecipeInstructions'].str.strip('()')\n",
    "\n",
    "# Write the modified DataFrame back to a new CSV file\n",
    "df.to_csv('tempo.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('tempo.csv')\n",
    "\n",
    "# Function to format instructions\n",
    "def format_instructions(instruction):\n",
    "    formatted_instruction = instruction.replace('\", \"', '\"\\n\"')\n",
    "    return formatted_instruction\n",
    "\n",
    "# Apply the function to each row of the 'RecipeInstructions' column\n",
    "df['RecipeInstructions'] = df['RecipeInstructions'].apply(format_instructions)\n",
    "\n",
    "# Write the modified DataFrame back to a new CSV file\n",
    "df.to_csv('tempo2.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv('tempo2.csv')\n",
    "\n",
    "# Check for rows containing the keywords and create a boolean mask\n",
    "keywords = ['mexican', 'american', 'italian', 'european']\n",
    "mask = df['merged_column'].str.contains('|'.join(keywords), case=False)\n",
    "\n",
    "# Remove the rows that meet the condition\n",
    "df = df[~mask]\n",
    "\n",
    "# Save the modified DataFrame back to a CSV file\n",
    "df.to_csv('tempo3.csv', index=False)\n",
    "\n",
    "print(\"Rows containing the keywords 'mexican', 'american', 'italian', or 'european' have been removed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('tempo3.csv')\n",
    "\n",
    "# Function to extract URLs from the string\n",
    "def extract_urls(row):\n",
    "    if isinstance(row, str):\n",
    "        urls = re.findall(r'https?://\\S+?\\.jpg', row)\n",
    "        return urls\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# Apply the function to each row\n",
    "df['urls'] = df['Images'].apply(extract_urls)\n",
    "\n",
    "# Function to check if URL is working\n",
    "def check_url(url):\n",
    "    try:\n",
    "        response = requests.head(url)\n",
    "        return url, response.status_code == 200\n",
    "    except requests.ConnectionError:\n",
    "        return url, False\n",
    "    except Exception as e:  # Catch any other exceptions\n",
    "        return url, False\n",
    "\n",
    "# Function to check multiple URLs asynchronously\n",
    "def check_multiple_urls_async(urls, row_numbers):\n",
    "    checked_rows = set()  # Set to store the checked row numbers\n",
    "    rows_deleted = 0  # Counter for deleted rows\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=20) as executor:  # Adjust max_workers as needed\n",
    "        future_to_url = {executor.submit(check_url, url): (url, row_number) for url, row_number in zip(urls, row_numbers)}\n",
    "        with tqdm(total=len(urls), position=0, leave=True) as pbar:  # Initialize tqdm progress bar\n",
    "            for future in concurrent.futures.as_completed(future_to_url):\n",
    "                url, row_number = future_to_url[future]\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    if not result[1]:  # If URL is not working\n",
    "                        print(f\"Non-working URL in Row {row_number}: {url}\")\n",
    "                        df.drop(df[df['urls'].apply(lambda x: url in x)].index, inplace=True)  # Delete entire row\n",
    "                        rows_deleted += 1  # Increment rows_deleted counter\n",
    "                    checked_rows.add(row_number)  # Add the checked row number to the set\n",
    "                    pbar.update(1)  # Update progress bar\n",
    "                except Exception as exc:\n",
    "                    print(f\"Error occurred while checking Row {row_number}: {exc}\")\n",
    "                    df.drop(df.index[df['urls'].apply(lambda x: url in x)], inplace=True)  # Delete entire row due to error\n",
    "                    rows_deleted += 1  # Increment rows_deleted counter\n",
    "    print(f\"Total Rows Checked: {len(checked_rows)}\")  # Print the total number of checked rows after completion\n",
    "    print(f\"Total Rows Deleted: {rows_deleted}\")  # Print the total number of deleted rows\n",
    "\n",
    "# Explode the URLs column and keep track of row numbers\n",
    "exploded_urls = df['urls'].explode()\n",
    "row_numbers = exploded_urls.index // len(df.columns) + 1  # Calculate row numbers based on index\n",
    "unique_urls = exploded_urls.unique()\n",
    "\n",
    "# Check if URLs are working asynchronously\n",
    "check_multiple_urls_async(unique_urls, row_numbers)\n",
    "\n",
    "# Save the updated DataFrame back to a new CSV file\n",
    "df.to_csv('tempo3_checked.csv', index=False)\n",
    "\n",
    "print(\"URL checking completed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
